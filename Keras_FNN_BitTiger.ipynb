{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codelab for Feedforward Neural Net\n",
    "\n",
    "All rights reserved.\n",
    "\n",
    "This material cannot be published, rewritten or redistributed in whole or part without the authors' written permission.\n",
    "\n",
    "During tutorial/workshop, attendees will be separated into three groups. Each group will be conducting different activities.\n",
    "\n",
    "Activity: \"Cell\" $\\rightarrow$ \"Run All\" for testing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'DT_RESOURCE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e71d66d4a619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown backend: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;31m# Define standard wrappers for the types_pb2.DataType enum.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m \u001b[0mresource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDT_RESOURCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0mfloat16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDT_HALF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0mhalf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'DT_RESOURCE'"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "#\n",
    "# All rights reserved.\n",
    "#\n",
    "# This is a codelab for Feedforward Neural Net.\n",
    "# Details include:\n",
    "#   - Pre-process dataset\n",
    "#   - Elaborate recipes\n",
    "#   - Define procedures\n",
    "#   - Train and test models\n",
    "#   - Observe metrics\n",
    "#\n",
    "################################################################\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras.callbacks as cb\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial/workshop activity 1: Pre-processing\n",
    "\n",
    "Each group performs different types of data pre-processing:<br />\n",
    "1. Group A proceed w/o pre-processing datasets.\n",
    "2. Group B proceed w/ normalizing datasets into the range of [0, 1].\n",
    "3. Group C proceed w/ standardizing datasets by z-scoring (de-mean, uni-variance).\n",
    "\n",
    "[See results](#Observe-Training-Process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PreprocessDataset():\n",
    "    from sklearn import preprocessing\n",
    "    ## Load dataset\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    ## Transform labels to one-hot\n",
    "    ## i.e., from '7' to [0,0,0,0,0,0,0,1,0,0]\n",
    "    y_train = np_utils.to_categorical(y_train, 10)\n",
    "    y_test = np_utils.to_categorical(y_test, 10)\n",
    "    \n",
    "    ## Process features. Set numeric type\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    ## Reshape from a matrix of 28 x 28 pixels to 1-D vector of 784 dimensions\n",
    "    x_train = np.reshape(x_train, (60000, 784))\n",
    "    x_test = np.reshape(x_test, (10000, 784))\n",
    "\n",
    "    ################################################################\n",
    "    # Activity 1 (Pre-processing):\n",
    "    # Group A: w/o pre-processing datasets.\n",
    "    #\n",
    "    # Group B: Min-Max Normalize value to [0, 1]\n",
    "    # x_train /= 255\n",
    "    # x_test /= 255\n",
    "    #\n",
    "    # Group C: proceed w/ standardizing datasets by z-scoring (de-mean, uni-variance).\n",
    "    # x_train = preprocessing.scale(x_train)\n",
    "    # x_test = preprocessing.scale(x_test)\n",
    "    ################################################################  \n",
    "    ## YOUR TURN: CHANGE HERE\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = PreprocessDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Show part of training data: features and labels\n",
    "## Each row is a sample, and each column represents a feature.\n",
    "print(\"{:^43}\".format(\"x\"), \"|\", \"{:^4}\".format(\"y\"))\n",
    "print(\"=\"*50)\n",
    "for sample_id in range(10):\n",
    "    print(\"{:.2f} {:.2f} ... {:.2f} {:.2f} {:.2f} ...  {:.2f} {:.2f}\".format(\n",
    "            x_train[sample_id][0], x_train[sample_id][1],\n",
    "            x_train[sample_id][156], x_train[sample_id][157], x_train[sample_id][158],\n",
    "            x_train[sample_id][-2], x_train[sample_id][-1]), \"| \",\n",
    "           \"{:.0f}\".format(y_train[sample_id][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipes for Neural Nets\n",
    "\n",
    "In this section you will play with useful recipes for designing a neural net.\n",
    "### Tutorial/workshop activity 2: Network Structure\n",
    "\n",
    "Each group uses different types of ingredients:<br />\n",
    "1. Group A uses **1** layer.\n",
    "2. Group B uses **2** layers of a **tower-shaped** (same width) network. \n",
    "3. Group C uses **2** layers of a **pyramid-shaped** (shrink width) network.\n",
    "\n",
    "[See results](#Observe-Training-Process)\n",
    "\n",
    "### Tutorial/workshop activity 3: Activation Function\n",
    "\n",
    "Each group uses different types of ingredients:\n",
    "1. Group A uses Relu.\n",
    "2. Group B uses Sigmoid.\n",
    "3. Group C uses Tanh.\n",
    "\n",
    "[See results](#Observe-Training-Process)\n",
    "\n",
    "### Tutorial/workshop activity 4: Loss Function\n",
    "\n",
    "Each group uses different types of ingredients:\n",
    "1. Group A uses cross entropy.\n",
    "2. Group B uses cross entropy.\n",
    "3. Group C uses squared error.\n",
    "\n",
    "[See results](#Observe-Training-Process)\n",
    "\n",
    "### Tutorial/workshop activity 5: Dropout\n",
    "\n",
    "Each group uses different types of ingredients:\n",
    "1. Group A uses 0% dropout.\n",
    "2. Group B uses 50% dropout.\n",
    "3. Group C uses 90% dropout.\n",
    "\n",
    "[See results](#Observe-Training-Process)\n",
    "\n",
    "### Tutorial/workshop activity 6: Regularization\n",
    "\n",
    "Each group uses different types of ingredients:\n",
    "1. Group A uses L1-norm.\n",
    "2. Group B uses L2-norm.\n",
    "3. Group C uses no regularization.\n",
    "\n",
    "[See results](#Observe-Training-Process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DefineModel():\n",
    "\n",
    "    ################################################################\n",
    "    # Activity 2 (Network Structure):\n",
    "    # Group A: uses only 1 layer\n",
    "    # second_layer_width = 0\n",
    "    #\n",
    "    # Group B: uses 2 layers of a tower-shaped (same width) network.\n",
    "    # second_layer_width = 128\n",
    "    #\n",
    "    # Group C: uses 2 layers of a pyramid-shaped (shrink width) network.\n",
    "    # second_layer_width = 64\n",
    "    ################################################################\n",
    "    first_layer_width = 128\n",
    "    second_layer_width = 64    \n",
    "    \n",
    "    ################################################################\n",
    "    # Activity 3 (Activation Function):\n",
    "    # Group A uses ReLU.\n",
    "    # activation_func = 'relu' \n",
    "    # \n",
    "    # Group B uses Sigmoid.\n",
    "    # activation_func = 'sigmoid'\n",
    "    #\n",
    "    # Group C uses Tanh.\n",
    "    # activation_func = 'tanh'\n",
    "    ################################################################\n",
    "    activation_func = 'relu' \n",
    "\n",
    "    ################################################################    \n",
    "    # Activity 4 (Loss Function):\n",
    "    # Group A uses cross entropy.\n",
    "    # loss_function = 'categorical_crossentropy'\n",
    "    # \n",
    "    # Group B uses cross entropy.\n",
    "    # loss_function = 'categorical_crossentropy'\n",
    "    # \n",
    "    # Group C uses squared error.\n",
    "    # loss_function = 'mean_squared_error'\n",
    "    ################################################################    \n",
    "    loss_function = 'categorical_crossentropy'\n",
    "    \n",
    "    #################################################################    \n",
    "    # Activity 5 (Dropout):\n",
    "    # Group A uses 0% dropout.\n",
    "    #\n",
    "    # Group B uses 50% dropout.\n",
    "    # dropout_rate = 0.5\n",
    "    #\n",
    "    # Group C uses 90% dropout.\n",
    "    # dropout_rate = 0.9\n",
    "    #################################################################    \n",
    "    dropout_rate = 0.0\n",
    "    \n",
    "    ################################################################    \n",
    "    # Activity 6 (Regularization):\n",
    "    # Group A uses L1 regularizer\n",
    "    # weight_regularizer = l1(0.01)\n",
    "    #\n",
    "    # Group B uses L2 regularizer\n",
    "    # weight_regularizer = l2(0.01)\n",
    "    # \n",
    "    # Group C uses no regularizer\n",
    "    # weight_regularizer = None\n",
    "    ################################################################\n",
    "    weight_regularizer = None\n",
    "\n",
    "    ################################################################    \n",
    "    # Activity 8 (Learning Rate):\n",
    "    # Group A uses learning rate of 0.1.\n",
    "    # learning_rate = 0.1\n",
    "    # \n",
    "    # Group B uses learning rate of 0.01.\n",
    "    # learning_rate = 0.01\n",
    "    #\n",
    "    # Group C uses learning rate of 0.5.    \n",
    "    # learning_rate = 0.5\n",
    "    ################################################################\n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    ## Initialize model.\n",
    "    model = Sequential()\n",
    "\n",
    "    ## First hidden layer with 'first_layer_width' neurons. \n",
    "    ## Also need to specify input dimension.\n",
    "    ## 'Dense' means fully-connected.\n",
    "    model.add(Dense(first_layer_width, input_dim=784, W_regularizer=weight_regularizer))\n",
    "    model.add(Activation(activation_func))\n",
    "    if dropout_rate > 0:\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "    ## Second hidden layer.\n",
    "    if second_layer_width > 0:\n",
    "        model.add(Dense(second_layer_width))\n",
    "        model.add(Activation(activation_func))\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(0.5))         \n",
    "    \n",
    "    ## Last layer has the same dimension as the number of classes\n",
    "    model.add(Dense(10))\n",
    "    ## For classification, the activation is softmax\n",
    "    model.add(Activation('softmax'))\n",
    "    ## Define optimizer. In this tutorial/codelab, we select SGD.\n",
    "    ## You can also use other methods, e.g., opt = RMSprop()\n",
    "    opt = SGD(lr=learning_rate, clipnorm=5.)\n",
    "    ## Define loss function = 'categorical_crossentropy' or 'mean_squared_error'\n",
    "    model.compile(loss=loss_function, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Procedure\n",
    "### Tutorial/workshop activity 7: Mini-batch\n",
    "\n",
    "Each group uses different types of ingredients:\n",
    "1. Group A uses mini-batch of size 128.\n",
    "2. Group B uses mini-batch of size 256.\n",
    "3. Group C uses mini-batch of size 512.\n",
    "\n",
    "[See results](#Observe-Training-Process)\n",
    "\n",
    "### Tutorial/workshop activity 8: Learning Rate\n",
    "\n",
    "Each group uses different types of ingredients:\n",
    "1. Group A uses learning rate of 0.1.\n",
    "2. Group B uses learning rate of 0.01.\n",
    "3. Group C uses learning rate of 0.5.\n",
    "\n",
    "[See results](#Observe-Training-Process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TrainModel(data=None, epochs=20):\n",
    "    ################################################################\n",
    "    # Activity 7 (Mini-batch):\n",
    "    # Group A uses mini-batch of size 128.\n",
    "    # batch = 128\n",
    "    #\n",
    "    # Group B uses mini-batch of size 256.\n",
    "    # batch = 256\n",
    "    # \n",
    "    # Group C uses mini-batch of size 512.\n",
    "    # batch = 512\n",
    "    ################################################################\n",
    "    batch=128\n",
    "    start_time = time.time()\n",
    "    model = DefineModel()\n",
    "    if data is None:\n",
    "        print(\"Must provide data.\")\n",
    "        return\n",
    "    x_train, x_test, y_train, y_test = data\n",
    "    print('Start training.')\n",
    "    ## Use the first 55,000 (out of 60,000) samples to train, last 5,500 samples to validate.\n",
    "    history = model.fit(x_train[:55000], y_train[:55000], nb_epoch=epochs, batch_size=batch,\n",
    "              validation_data=(x_train[55000:], y_train[55000:]))\n",
    "    print(\"Training took {0} seconds.\".format(time.time() - start_time))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_model, training_history = TrainModel(data=[x_train, x_test, y_train, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PlotHistory(train_value, test_value, value_is_loss_or_acc):\n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot([None] + train_value, 'o-')\n",
    "    ax.plot([None] + test_value, 'x-')\n",
    "    ## Plot legend and use the best location automatically: loc = 0.\n",
    "    ax.legend(['Train ' + value_is_loss_or_acc, 'Validation ' + value_is_loss_or_acc], loc = 0) \n",
    "    ax.set_title('Training/Validation ' + value_is_loss_or_acc + ' per Epoch')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(value_is_loss_or_acc)  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PlotHistory(training_history.history['loss'], training_history.history['val_loss'], 'Loss')\n",
    "PlotHistory(training_history.history['acc'], training_history.history['val_acc'], 'Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe Regularization Effects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drawWeightHistogram(x):\n",
    "    ## the histogram of the data\n",
    "    fig = plt.subplots()\n",
    "    n, bins, patches = plt.hist(x, 50)\n",
    "    plt.xlim(-0.5, 0.5)\n",
    "    plt.xlabel('Weight')\n",
    "    plt.ylabel('Count')\n",
    "    zero_counts = (x == 0.0).sum()\n",
    "    plt.title(\"Weight Histogram. Num of '0's: %d\" % zero_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w1 = trained_model.layers[0].get_weights()[0].flatten()\n",
    "drawWeightHistogram(w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Testing Procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TestModel(model=None, data=None):\n",
    "    if model is None:\n",
    "        print(\"Must provide a trained model.\")\n",
    "        return\n",
    "    if data is None:\n",
    "        print(\"Must provide data.\")\n",
    "        return\n",
    "    x_test, y_test = data\n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_score = TestModel(model=trained_model, data=[x_test, y_test])\n",
    "print(\"Test loss {:.4f}, accuracy {:.2f}%\".format(test_score[0], test_score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ShowInputImage(data):\n",
    "    \"\"\"Visualize input image.\"\"\"\n",
    "    plot = plt.figure()\n",
    "    plot.set_size_inches(2,2)\n",
    "    plt.imshow(np.reshape(-data, (28,28)), cmap='Greys_r')\n",
    "    plt.title(\"Input\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def ShowHiddenLayerOutput(input_data, target_layer_num):\n",
    "    \"\"\"Visualize output from the target hidden layer.\"\"\"\n",
    "    from keras import backend as K\n",
    "    ## Backend converter: to TensorFlow\n",
    "    target_layer = K.function(trained_model.inputs, [trained_model.layers[target_layer_num].output])\n",
    "    ## Extract output from the target hidden layer.\n",
    "    target_layer_out = target_layer([input_data])\n",
    "    plot = plt.figure()\n",
    "    plot.set_size_inches(2,2)\n",
    "    plt.imshow(np.reshape(-target_layer_out[0][0], (16,-1)), cmap='Greys_r')\n",
    "    plt.title(\"Hidden layer \" + str(target_layer_num))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def ShowFinalOutput(input_data):\n",
    "    \"\"\"Calculate final prediction.\"\"\"\n",
    "    from keras import backend as K\n",
    "    ## Backend converter: to TensorFlow\n",
    "    ## Calculate final prediction.\n",
    "    last_layer = K.function(trained_model.inputs, [trained_model.layers[-1].output])\n",
    "    last_layer_out = last_layer([input_data])\n",
    "    print(\"Final prediction: \" + str(np.argmax(last_layer_out[0][0])) )\n",
    "\n",
    "ShowInputImage(x_test[0])\n",
    "ShowHiddenLayerOutput(x_test, 1)\n",
    "ShowFinalOutput(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
